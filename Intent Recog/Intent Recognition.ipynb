{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84a198a1",
   "metadata": {},
   "source": [
    "### AIM: To identify and classify the intent of a given text into the pre-defined 5 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "232d08bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbert\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import bert\n",
    "from bert import BertModelLayer\n",
    "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n",
    "from bert.tokenization.bert_tokenization import FullTokenizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af3dbd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "valid = pd.read_csv(\"valid.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c908137",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.append(valid).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d8172dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13784, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6d62a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>listen to westbam alumb allergic on google music</td>\n",
       "      <td>PlayMusic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>add step to me to the 50 clásicos playlist</td>\n",
       "      <td>AddToPlaylist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i give this current textbook a rating value of...</td>\n",
       "      <td>RateBook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>play the song little robin redbreast</td>\n",
       "      <td>PlayMusic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>please add iris dement to my playlist this is ...</td>\n",
       "      <td>AddToPlaylist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text         intent\n",
       "0   listen to westbam alumb allergic on google music      PlayMusic\n",
       "1         add step to me to the 50 clásicos playlist  AddToPlaylist\n",
       "2  i give this current textbook a rating value of...       RateBook\n",
       "3               play the song little robin redbreast      PlayMusic\n",
       "4  please add iris dement to my playlist this is ...  AddToPlaylist"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9355b3a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d7507fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3291cc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  uncased_L-12_H-768_A-12.zip\n",
      "   creating: uncased_L-12_H-768_A-12/\n",
      "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
      "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
      "  inflating: uncased_L-12_H-768_A-12/vocab.txt  \n",
      "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
      "  inflating: uncased_L-12_H-768_A-12/bert_config.json  \n"
     ]
    }
   ],
   "source": [
    "!unzip uncased_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7d3daf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"model\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6099a773",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv uncased_L-12_H-768_A-12/ model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c23855c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_name=\"uncased_L-12_H-768_A-12\"\n",
    "\n",
    "bert_ckpt_dir = os.path.join(\"model/\", bert_model_name)\n",
    "bert_ckpt_file = os.path.join(bert_ckpt_dir, \"bert_model.ckpt\")\n",
    "bert_config_file = os.path.join(bert_ckpt_dir, \"bert_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad9093d",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7670749",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntentDetectionData:\n",
    "    DATA_COLUMN = \"text\"\n",
    "    LABEL_COLUMN = \"intent\"\n",
    "\n",
    "    def __init__(self, train, test, tokenizer: FullTokenizer, classes, max_seq_len=192):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = 0\n",
    "        self.classes = classes\n",
    "    \n",
    "        train, test = map(lambda df: df.reindex(df[IntentDetectionData.DATA_COLUMN].str.len().sort_values().index), [train, test])\n",
    "    \n",
    "        ((self.train_x, self.train_y), (self.test_x, self.test_y)) = map(self._prepare, [train, test])\n",
    "\n",
    "        print(\"max seq_len\", self.max_seq_len)\n",
    "        self.max_seq_len = min(self.max_seq_len, max_seq_len)\n",
    "        self.train_x, self.test_x = map(self._pad, [self.train_x, self.test_x])\n",
    "\n",
    "    def _prepare(self, df):\n",
    "        x, y = [], []\n",
    "    \n",
    "        for _, row in tqdm(df.iterrows()):\n",
    "            text, label = row[IntentDetectionData.DATA_COLUMN], row[IntentDetectionData.LABEL_COLUMN]\n",
    "            tokens = self.tokenizer.tokenize(text)\n",
    "            tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "            token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "            self.max_seq_len = max(self.max_seq_len, len(token_ids))\n",
    "            x.append(token_ids)\n",
    "            y.append(self.classes.index(label))\n",
    "\n",
    "        return np.array(x), np.array(y)\n",
    "\n",
    "    def _pad(self, ids):\n",
    "        x = []\n",
    "        for input_ids in ids:\n",
    "            input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)]\n",
    "            input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids))\n",
    "            x.append(np.array(input_ids))\n",
    "        return np.array(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c5a90303",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = FullTokenizer(vocab_file=os.path.join(bert_ckpt_dir, \"vocab.txt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c6a1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokens = tokenizer.tokenize(\"I can't wait to meet my best friend!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05264789",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.convert_tokens_to_ids(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9df83a",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b3e4c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_seq_len, bert_ckpt_file):\n",
    "\n",
    "    with tf.io.gfile.GFile(bert_config_file, \"r\") as reader:\n",
    "        bc = StockBertConfig.from_json_string(reader.read())\n",
    "        bert_params = map_stock_config_to_params(bc)\n",
    "        bert_params.adapter_size = None\n",
    "        bert = BertModelLayer.from_params(bert_params, name=\"bert\")\n",
    "        \n",
    "    input_ids = keras.layers.Input(shape=(max_seq_len, ), dtype='int32', name=\"input_ids\")\n",
    "    bert_output = bert(input_ids)\n",
    "\n",
    "    print(\"bert shape\", bert_output.shape)\n",
    "\n",
    "    cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(bert_output)\n",
    "    cls_out = keras.layers.Dropout(0.5)(cls_out)\n",
    "    logits = keras.layers.Dense(units=768, activation=\"tanh\")(cls_out)\n",
    "    logits = keras.layers.Dropout(0.5)(logits)\n",
    "    logits = keras.layers.Dense(units=len(classes), activation=\"softmax\")(logits)\n",
    "\n",
    "    model = keras.Model(inputs=input_ids, outputs=logits)\n",
    "    model.build(input_shape=(None, max_seq_len))\n",
    "\n",
    "    load_stock_weights(bert, bert_ckpt_file)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96e377c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = train.intent.unique().tolist()\n",
    "#print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ecffd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13784it [00:01, 7284.27it/s]\n",
      "/tmp/ipykernel_7932/3736353746.py:30: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(x), np.array(y)\n",
      "700it [00:00, 7005.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max seq_len 38\n"
     ]
    }
   ],
   "source": [
    "data = IntentDetectionData(train, test, tokenizer, classes, max_seq_len=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87f7ba8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13784, 38)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84044e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 101, 2377, 3769,  102,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_x[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf07a245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b67fa02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.max_seq_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5c63f4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 18:02:13.680705: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-04-04 18:02:13.680779: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-04-04 18:02:13.680845: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (hb-yogesh): /proc/driver/nvidia/version does not exist\n",
      "2023-04-04 18:02:13.698702: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2023-04-04 18:02:13.911159: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3600000000 Hz\n",
      "2023-04-04 18:02:13.912390: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fef20000b60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-04-04 18:02:13.912454: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-04-04 18:02:13.986149: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 93763584 exceeds 10% of free system memory.\n",
      "2023-04-04 18:02:14.167058: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 93763584 exceeds 10% of free system memory.\n",
      "2023-04-04 18:02:14.228301: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 93763584 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert shape (None, 38, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 18:02:19.015976: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 93763584 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading 196 BERT weights from: model/uncased_L-12_H-768_A-12/bert_model.ckpt into <bert.model.BertModelLayer object at 0x7ff00dae52b0> (prefix:bert). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tbert/embeddings/token_type_embeddings\n",
      "\tbert/pooler/dense/bias\n",
      "\tbert/pooler/dense/kernel\n",
      "\tcls/predictions/output_bias\n",
      "\tcls/predictions/transform/LayerNorm/beta\n",
      "\tcls/predictions/transform/LayerNorm/gamma\n",
      "\tcls/predictions/transform/dense/bias\n",
      "\tcls/predictions/transform/dense/kernel\n",
      "\tcls/seq_relationship/output_bias\n",
      "\tcls/seq_relationship/output_weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 18:02:30.757791: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 93763584 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "model = create_model(data.max_seq_len, bert_ckpt_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a072e39f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_ids (InputLayer)       [(None, 38)]              0         \n",
      "_________________________________________________________________\n",
      "bert (BertModelLayer)        (None, 38, 768)           108890112 \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 768)               590592    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 5383      \n",
      "=================================================================\n",
      "Total params: 109,486,087\n",
      "Trainable params: 109,486,087\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f44d290e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(1e-5),\n",
    "  loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "974f3b18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 18:02:31.313085: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "  1/776 [..............................] - ETA: 0s - loss: 1.9628 - acc: 0.1250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 18:03:00.188716: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session started.\n",
      "2023-04-04 18:03:08.183816: I tensorflow/core/profiler/rpc/client/save_profile.cc:168] Creating directory: log/intent_detection/20230404-18021680611551/train/plugins/profile/2023_04_04_18_03_08\n",
      "2023-04-04 18:03:08.349228: I tensorflow/core/profiler/rpc/client/save_profile.cc:174] Dumped gzipped tool data for trace.json.gz to log/intent_detection/20230404-18021680611551/train/plugins/profile/2023_04_04_18_03_08/hb-yogesh.trace.json.gz\n",
      "2023-04-04 18:03:08.422256: I tensorflow/core/profiler/utils/event_span.cc:288] Generation of step-events took 1.12 ms\n",
      "\n",
      "2023-04-04 18:03:09.657119: I tensorflow/python/profiler/internal/profiler_wrapper.cc:87] Creating directory: log/intent_detection/20230404-18021680611551/train/plugins/profile/2023_04_04_18_03_08Dumped tool data for overview_page.pb to log/intent_detection/20230404-18021680611551/train/plugins/profile/2023_04_04_18_03_08/hb-yogesh.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to log/intent_detection/20230404-18021680611551/train/plugins/profile/2023_04_04_18_03_08/hb-yogesh.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to log/intent_detection/20230404-18021680611551/train/plugins/profile/2023_04_04_18_03_08/hb-yogesh.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to log/intent_detection/20230404-18021680611551/train/plugins/profile/2023_04_04_18_03_08/hb-yogesh.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "776/776 [==============================] - 2133s 3s/step - loss: 1.3115 - acc: 0.8676 - val_loss: 1.1724 - val_acc: 0.9920\n",
      "Epoch 2/2\n",
      "776/776 [==============================] - 2069s 3s/step - loss: 1.1852 - acc: 0.9817 - val_loss: 1.1701 - val_acc: 0.9949\n"
     ]
    }
   ],
   "source": [
    "log_dir = \"log/intent_detection/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%s\")\n",
    "\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "\n",
    "history = model.fit(x=data.train_x, y=data.train_y,validation_split=0.1,batch_size=16,shuffle=True,epochs=2,\n",
    "                    callbacks=[tensorboard_callback])\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "69d9858f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "431/431 [==============================] - 516s 1s/step - loss: 1.1749 - acc: 0.9905\n",
      "22/22 [==============================] - 24s 1s/step - loss: 1.1866 - acc: 0.9786\n",
      "train acc 0.9904962182044983\n",
      "test acc 0.9785714149475098\n"
     ]
    }
   ],
   "source": [
    "_, train_acc = model.evaluate(data.train_x, data.train_y)\n",
    "_, test_acc = model.evaluate(data.test_x, data.test_y)\n",
    "\n",
    "print(\"train acc\", train_acc)\n",
    "print(\"test acc\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "938ac126",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(data.test_x).argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81941a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      precision    recall  f1-score   support\n",
      "\n",
      "           PlayMusic       0.98      0.98      0.98        86\n",
      "       AddToPlaylist       0.99      1.00      1.00       124\n",
      "            RateBook       1.00      1.00      1.00        80\n",
      "SearchScreeningEvent       0.98      0.93      0.96       107\n",
      "      BookRestaurant       0.98      1.00      0.99        92\n",
      "          GetWeather       1.00      0.98      0.99       104\n",
      "  SearchCreativeWork       0.93      0.96      0.94       107\n",
      "\n",
      "            accuracy                           0.98       700\n",
      "           macro avg       0.98      0.98      0.98       700\n",
      "        weighted avg       0.98      0.98      0.98       700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(data.test_y, y_pred, target_names=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef629e8c",
   "metadata": {},
   "source": [
    "# Testing the trained Model with unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "827211f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: play the song on high volume. \n",
      "intent: PlayMusic\n",
      "\n",
      "text: This book was awful \n",
      "intent: RateBook\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"play the song on high volume.\",\"This book was awful\"]\n",
    "\n",
    "pred_tokens = map(tokenizer.tokenize, sentences)\n",
    "pred_tokens = map(lambda tok: [\"[CLS]\"] + tok + [\"[SEP]\"], pred_tokens)\n",
    "pred_token_ids = list(map(tokenizer.convert_tokens_to_ids, pred_tokens))\n",
    "\n",
    "pred_token_ids = [tids + [0] * (data.max_seq_len - len(tids)) for tids in pred_token_ids]\n",
    "pred_token_ids = np.array(list(pred_token_ids))\n",
    "\n",
    "predictions = model.predict(pred_token_ids).argmax(axis=-1)\n",
    "\n",
    "for text, label in zip(sentences, predictions):\n",
    "    print(\"text:\", text, \"\\nintent:\", classes[label])\n",
    "    print()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "09ca8c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"my_model.h5\")\n",
    "model.save('my_model.h5', save_format='h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08bd03b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PlayMusic', 'AddToPlaylist', 'RateBook', 'SearchScreeningEvent', 'BookRestaurant', 'GetWeather', 'SearchCreativeWork']\n"
     ]
    }
   ],
   "source": [
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b1c144",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
